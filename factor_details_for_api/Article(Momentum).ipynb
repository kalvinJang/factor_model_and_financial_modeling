{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44db92c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import pickle, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm.notebook as tqdm\n",
    "import copy\n",
    "from scipy import stats\n",
    "from scipy.stats.mstats import winsorize\n",
    "from statsmodels.formula.api import ols\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\jky93\\\\KYdrive\\\\바탕 화면\\\\창업\\\\팩터프로젝트')\n",
    "# # Data Download\n",
    "with open('FS_손익계산서보정본', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "with open('Final_FS_ind', 'rb') as f:\n",
    "    ind = pickle.load(f)\n",
    "\n",
    "# 특정 자산만 뽑아서 csv로 정리하기\n",
    "def get_item(concept_id):\n",
    "    item = []\n",
    "    item_ind = []\n",
    "    # check id's address\n",
    "    id_address = None\n",
    "    for i in range(len(data)):\n",
    "        for j in range(3):\n",
    "            try:\n",
    "                if concept_id in data[i][j].iloc[:,0].values:\n",
    "                    id_address = j\n",
    "            except AttributeError:\n",
    "                pass\n",
    "        if id_address != None:\n",
    "            break\n",
    "            \n",
    "    for i in range(len(data)):\n",
    "        if type(data[i][id_address]) == type(data[0][id_address]):\n",
    "            item.append(data[i][id_address][data[i][id_address][data[i][id_address].columns[0]] == concept_id].iloc[:1])\n",
    "            if len(data[i][id_address][data[i][id_address][data[i][id_address].columns[0]] == concept_id]) > 0:\n",
    "                item_ind.append(ind[i])\n",
    "    df_item = pd.concat(item)\n",
    "    date = list(filter(lambda x: x[1] == ('연결재무제표',), df_item.columns))\n",
    "    if id_address ==0:\n",
    "        date_clean = list(map(lambda x: x[0], df_item[date].columns))\n",
    "        df_item[date].columns = date_clean\n",
    "        df_item2 = df_item[date]\n",
    "        df_item2.columns = date_clean\n",
    "        date_shifted = list(map(lambda x: shift_q(x), date_clean))\n",
    "        df_item3 = df_item2\n",
    "        df_item3.columns = date_shifted\n",
    "        df_item4 = df_item3.groupby(level=0, axis=1).last()\n",
    "        df_item3 = df_item4\n",
    "        date_shifted2 = list(map(lambda x: shift_d(x), df_item4.columns))\n",
    "        df_item3.columns = date_shifted2\n",
    "        df_item5 = df_item3.groupby(level=0, axis=1).last()\n",
    "        corp_code = list(map(lambda x:x[2], item_ind))\n",
    "        df_item5['corp_code'] = corp_code\n",
    "        df_item5 = df_item5.set_index('corp_code')\n",
    "    else:\n",
    "        date_clean = list(map(lambda x: x[0], df_item[date].columns))\n",
    "        df_item[date].columns = date_clean\n",
    "        df_item2 = df_item[date]\n",
    "        df_item2.columns = date_clean\n",
    "        date_shifted = list(map(lambda x: shift_flow_date(x), date_clean))\n",
    "        df_item2.columns = date_shifted\n",
    "        df_item3 = df_item2.groupby(level=0, axis=1).last()\n",
    "        corp_code = list(map(lambda x:x[2], item_ind))\n",
    "        df_item3['corp_code'] = corp_code\n",
    "        df_item3 = df_item3.set_index('corp_code')\n",
    "        df_item5 = df_item3\n",
    "    return df_item5\n",
    "\n",
    "def shift_q(date):\n",
    "    if date[4:6]=='01' or date[4:6]=='02' or date[4:6]=='03':\n",
    "        date = date[:4] + '0331'\n",
    "    elif date[4:6]=='04' or date[4:6]=='05' or date[4:6]=='06':\n",
    "        date = date[:4] + '0630'\n",
    "    elif date[4:6]=='07' or date[4:6]=='08' or date[4:6]=='09':\n",
    "        date = date[:4] + '0930'\n",
    "    elif date[4:6]=='10' or date[4:6]=='11' or date[4:6]=='12':\n",
    "        date = date[:4] + '1231'\n",
    "    return date\n",
    "\n",
    "def shift_d(date):\n",
    "    # 케이스 두개 밖에 안되니까 그냥 수동으로 따져주자.\n",
    "    if date[6:]=='01':\n",
    "        date = date[:6] + '31'\n",
    "    elif date[6:]=='29':\n",
    "        date = date[:6] + '30'\n",
    "    return date\n",
    "\n",
    "def shift_flow_date(date):\n",
    "    # 우선은 연간 데이터만 활용하도록 만들어놓아도 될 듯? 나중에 분기별로 뽑을 수 있는 옵션을 추가해야 할 것 같다.\n",
    "    if date[4:6]=='01' and date[-4:-2]=='12':\n",
    "        date = date[-8:-2] + '31'\n",
    "    else:\n",
    "        date = None\n",
    "    return date\n",
    "\n",
    "def get_item_on_date(item, date):\n",
    "    return get_item(item)[date]\n",
    "\n",
    "def get_item_for_period(item, date_from, date_to, term = 'Y'):\n",
    "    # term은 Y, H, Q의 세 가지 옵션으로, 연도별, 반기별, 분기별 데이터를 얻어올 수 있도록 하는 옵션이다.\n",
    "    data = []\n",
    "    item_data = get_item(item)\n",
    "    if term == 'Y':\n",
    "        num = int(date_to[:4]) - int(date_from[:4]) +1\n",
    "        date_working = date_from\n",
    "        for i in range(num):\n",
    "            data.append(item_data[date_working])\n",
    "            date_working = date_working[:2] + str(int(date_working[2:4]) + 1) + date_working[4:]\n",
    "            \n",
    "    elif term == 'H':\n",
    "        num = int((int(date_to[:4]) - int(date_from[:4])) * 2 + (int(date_to[4:6]) - int(date_from[4:6]))/6 + 1)\n",
    "        date_working = date_from\n",
    "        for i in range(num):\n",
    "            try:\n",
    "                data.append(item_data[date_working])\n",
    "            except:\n",
    "                try:\n",
    "                    date_working = date_working[:6] + str(61 - int(date_working[6:]))\n",
    "                    data.append(item_data[date_working])\n",
    "                except KeyError:\n",
    "                    print('해당 날짜의 데이터가 없습니다:', date_working)\n",
    "            if int(date_working[4:6])+6 > 12:\n",
    "                date_working = date_working[:2] + str(int(date_working[2:4]) + 1) +'0'+ str(int(date_working[4:6])-6)+'31'\n",
    "            else:\n",
    "                date_working = date_working[:4] + str(int(date_working[4:6]) + 6).zfill(2) + '31'\n",
    "                \n",
    "    else:\n",
    "        num = int((int(date_to[:4]) - int(date_from[:4])) * 4 + (int(date_to[4:6]) - int(date_from[4:6]))/3 + 1)\n",
    "        date_working = date_from\n",
    "        for i in range(num):\n",
    "            try:\n",
    "                data.append(item_data[date_working])\n",
    "            except:\n",
    "                try:\n",
    "                    date_working = date_working[:6] + str(61 - int(date_working[6:]))\n",
    "                    data.append(item_data[date_working])\n",
    "                except KeyError:\n",
    "                    print('해당 날짜의 데이터가 없습니다:', date_working)\n",
    "            if int(date_working[4:6])+3 > 12:\n",
    "                date_working = date_working[:2] + str(int(date_working[2:4]) + 1) +'0331'\n",
    "            else:\n",
    "                date_working = date_working[:4] + str(int(date_working[4:6]) + 3).zfill(2) + '31'\n",
    "                \n",
    "    data = pd.concat(data, axis = 1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_market_data_on_date(date, print_error = True):\n",
    "    # 에러 메시지를 출력하고 싶지 않으면 print_error를 False로 설정\n",
    "    os.chdir('C:\\\\Users\\\\jky93\\\\KYdrive\\\\바탕 화면\\\\창업\\\\팩터프로젝트\\\\KRX_price')\n",
    "    try:\n",
    "        data = pd.read_csv(date[2:4] + '_' + str(int(date[4:6])) + '_' + str(int(date[6:8])) + '.csv', encoding = 'cp949')\n",
    "        data['종목코드'] = data['종목코드'].map(lambda x:str(x).zfill(6) if x != None else None)\n",
    "        if data.isna()['종가'].sum() == len(data):\n",
    "            raise FileNotFoundError\n",
    "    except FileNotFoundError:\n",
    "        if print_error:\n",
    "            print('해당 날짜의 데이터가 존재하지 않습니다:', date)\n",
    "        data = None\n",
    "    return data\n",
    "\n",
    "def get_market_cap_on_date(date, get_code = False):\n",
    "    if get_code:\n",
    "        data = get_market_data_on_date(date)[['종목코드','시장구분','시가총액']].set_index('종목코드')\n",
    "        data.columns = [['시장구분','Market_Cap']]\n",
    "    else:\n",
    "        data = get_market_data_on_date(date)[['종목코드','시가총액']].set_index('종목코드')\n",
    "        data.columns = [['Market_Cap']]\n",
    "    data.index.rename('corp_code', inplace = True)\n",
    "    return data\n",
    "\n",
    "def last_market_date(date):\n",
    "    # date의 기본형식은 'YYYYMM' : ex)'201212'. -> 'YYYYMMDD'도 가능하도록 코드 변경했음\n",
    "    get_data = False\n",
    "    day = 31\n",
    "    while not get_data:\n",
    "        if type(get_market_data_on_date(date[:6] + str(day), print_error = False)) != type(None):\n",
    "            market_date = date[:6] + str(day)\n",
    "            get_data = True\n",
    "        else:\n",
    "            day -=1\n",
    "    return market_date\n",
    "\n",
    "def find_next_market_date(date, today_opt = False):\n",
    "    get_data = False\n",
    "    if today_opt:\n",
    "        market_date = pd.Timestamp(date)\n",
    "    else:\n",
    "        market_date = pd.Timestamp(date) + pd.Timedelta(days=1)\n",
    "    while not get_data:\n",
    "        if type(get_market_data_on_date(market_date.strftime('%Y%m%d'), print_error = False)) != type(None):\n",
    "            get_data = True\n",
    "        else:\n",
    "            market_date += pd.Timedelta(days=1)\n",
    "    \n",
    "    return market_date.strftime('%Y%m%d')\n",
    "\n",
    "def get_market_cap_for_period(date_from, date_to, term = 'Y', ind_match = True):\n",
    "    # ind_match는 fundamental과 합치기 편하도록 실제 날짜가 아니라 월말 날짜를 표시하도록 하는 옵션\n",
    "    data = []\n",
    "    ind = []\n",
    "    if term == 'Y':\n",
    "        num = int(date_to[:4]) - int(date_from[:4]) +1\n",
    "        date_working = date_from\n",
    "        for i in range(num):\n",
    "            data.append(get_market_cap_on_date(last_market_date(date_working), get_code = True))\n",
    "            if ind_match:\n",
    "                ind.append(date_working)\n",
    "            else:\n",
    "                ind.append(last_market_date(date_working))\n",
    "            date_working = date_working[:2] + str(int(date_working[2:4]) + 1) + date_working[4:]\n",
    "            \n",
    "    elif term == 'H':\n",
    "        num = int((int(date_to[:4]) - int(date_from[:4])) * 2 + (int(date_to[4:6]) - int(date_from[4:6]))/6 + 1)\n",
    "        date_working = date_from\n",
    "        for i in range(num):\n",
    "            data.append(get_market_cap_on_date(last_market_date(date_working), get_code = True))\n",
    "            if ind_match:\n",
    "                ind.append(date_working)\n",
    "            else:\n",
    "                ind.append(last_market_date(date_working))\n",
    "            if int(date_working[4:6])+6 > 12:\n",
    "                date_working = date_working[:2] + str(int(date_working[2:4]) + 1) +'0'+ str(int(date_working[4:6])-6)+'31'\n",
    "            else:\n",
    "                date_working = date_working[:4] + str(int(date_working[4:6]) + 6).zfill(2) + '31'\n",
    "                \n",
    "    else:\n",
    "        num = int((int(date_to[:4]) - int(date_from[:4])) * 4 + (int(date_to[4:6]) - int(date_from[4:6]))/3 + 1)\n",
    "        date_working = date_from\n",
    "        for i in range(num):\n",
    "            data.append(get_market_cap_on_date(last_market_date(date_working), get_code = True))\n",
    "            if ind_match:\n",
    "                ind.append(date_working)\n",
    "            else:\n",
    "                ind.append(last_market_date(date_working))\n",
    "            if int(date_working[4:6])+3 > 12:\n",
    "                date_working = date_working[:2] + str(int(date_working[2:4]) + 1) +'0331'\n",
    "            else:\n",
    "                date_working = date_working[:4] + str(int(date_working[4:6]) + 3).zfill(2) + '31'\n",
    "    cap_data = [x.iloc[:,1] for x in data]\n",
    "    cat_data = [x.iloc[:,0] for x in data]\n",
    "    cap_data = pd.concat(cap_data, axis =1)\n",
    "    cap_data.columns = ind\n",
    "    cat_data = pd.concat(cat_data, axis =1)\n",
    "    cat_data.columns = ind\n",
    "    return cap_data, cat_data\n",
    "\n",
    "def get_market_category(data):\n",
    "    market_cat = []\n",
    "    for i in range(len(data.columns)):\n",
    "        market_cat.append(get_market_cap_on_date(last_market_date(data.columns[i]), get_code = True).iloc[:,0])\n",
    "    market_cat = pd.concat(market_cat, axis=1)\n",
    "    market_cat.columns = data.columns\n",
    "    return market_cat\n",
    "\n",
    "def get_breakpoint(data, breakpoint):\n",
    "    absolute_point = []\n",
    "    rank = data.rank(pct=True)\n",
    "    for i in range(len(breakpoint)):\n",
    "        absolute_point.append((data[rank < breakpoint[i]].max() + data[rank > breakpoint[i]].min())/2)\n",
    "    return absolute_point\n",
    "\n",
    "def get_mask(data, market_cat = None, market_bp = 'All', breakpoint: list = [0.5]):\n",
    "    # data는 mask의 기준이 되는 baseline data\n",
    "    # market_bp는 'ALL', 'KOSPI', 'KOSDAQ'으로, 기준이 되는 breakpoint의 market을 의미\n",
    "    # breakpoint는 masking의 구분점으로, list안에 float이 담긴 형태를 받음\n",
    "    breakpoint = np.sort(breakpoint)\n",
    "    mask_list = []\n",
    "    \n",
    "    data_ind = data.index\n",
    "    if type(market_cat) != type(None):\n",
    "        market_cat = market_cat[market_cat.index.map(lambda x: x in data.index)]\n",
    "        for i in range(len(data)):\n",
    "            if data_ind[i] not in market_cat.index:\n",
    "                market_cat.loc[data_ind[i]] = None\n",
    "        market_cat = market_cat.reindex(index = data.index)\n",
    "    for i in range(len(data.columns)):\n",
    "        mask = data.iloc[:,i] * 0 +1\n",
    "        mask_np = -np.ones(len(mask))\n",
    "        #rank = data.iloc[:,i].rank(pct = True)\n",
    "        if market_bp == 'All':\n",
    "            absolute_point = get_breakpoint(data.iloc[:,i], breakpoint)\n",
    "        if market_bp == 'KOSPI':\n",
    "            absolute_point = get_breakpoint(data[market_cat.iloc[:,i]=='KOSPI'].iloc[:,i], breakpoint)\n",
    "        if market_bp == 'KOSDAQ':\n",
    "            absolute_point = get_breakpoint(data[market_cat.iloc[:,i]=='KOSDAQ'].iloc[:,i], breakpoint)\n",
    "        for j in range(len(breakpoint)+1):\n",
    "            if j < len(breakpoint):\n",
    "                for k in range(len(mask)):\n",
    "                    mask_np[k] = j if data.iloc[k,i] < absolute_point[j] and mask_np[k] == -1 else mask_np[k]\n",
    "            else:\n",
    "                for k in range(len(mask)):\n",
    "                    mask_np[k] = j if data.iloc[k,i] > absolute_point[j-1] and mask_np[k] == -1 else mask_np[k]\n",
    "        mask = mask * mask_np\n",
    "        mask_list.append(mask)\n",
    "    mask_df = pd.concat(mask_list, axis =1)\n",
    "    return mask_df\n",
    "\n",
    "def shift_date_quarter(data, num_of_quarters):\n",
    "    # data의 date를 원하는 만큼 밀어줌.\n",
    "    dates = data.columns\n",
    "    years = [int(x[:4]) for x in dates]\n",
    "    months = [int(x[4:6]) for x in dates]\n",
    "    shifted_months = [x + num_of_quarters * 3 for x in months]\n",
    "    shifted_years = [x + (shifted_months[i]-1) // 12 for i, x in enumerate(years)]\n",
    "    shifted_months = [(x -1) % 12 +1 for x in shifted_months]\n",
    "    dates = [str(shifted_years[i]) + str(shifted_months[i]).zfill(2)+ '30' if shifted_months[i] in [6, 9] else str(shifted_years[i]) + str(shifted_months[i]).zfill(2)+ '31' for i in np.arange(len(years))]\n",
    "    data2 = copy.deepcopy(data)\n",
    "    data2.columns = dates\n",
    "    return data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8e65b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_factor_on_date_by_mask(mask, term = 'd', winsorize_limits = 0.01, weight = 'EW'):\n",
    "    # @@@@@@@@@@@ option 추가해야할듯! : daily / weekly / monthly / quarterly 정도까지는 구현을 해놓아야할듯\n",
    "    # monthly 계산할 때 첫 달은 어떻게 계산하는거지? 전달 말일 기준으로 계산하나? 아니면 해당 달 시초가 기준으로 계산하나?\n",
    "    # -> 우선 첫 달은 시가 기준, 이후로는 전달 말일에 리밸런싱 하는 걸 기준으로 계산했음. \n",
    "    term_conservative = pd.Timestamp(mask.columns[1]) - pd.Timestamp(mask.columns[0]) - pd.Timedelta(days = 7)\n",
    "    if pd.Timestamp(mask.columns[-1]) + term_conservative >  pd.Timestamp('20210630'):\n",
    "        date_to = pd.Timestamp('20210630')\n",
    "    else:\n",
    "        date_to = pd.Timestamp(last_market_date((pd.Timestamp(mask.columns[-1]) + term_conservative).strftime('%Y%m%d')))\n",
    "    date_from = pd.Timestamp(mask.columns[0])\n",
    "    factor_days = (date_to - date_from).days\n",
    "    factor_months = (date_to.year - date_from.year)*12 + (date_to.month - date_from.month)\n",
    "    \n",
    "    date_point = date_from.strftime('%Y%m%d')\n",
    "    # this is for storing the last date\n",
    "    date_point_marked = None\n",
    "    \n",
    "    total_factor = []\n",
    "    date_list = []\n",
    "    \n",
    "    if term == 'd':\n",
    "        flag = -1\n",
    "#         for _ in range(factor_days):\n",
    "        for _ in tqdm.tqdm(range(10)):\n",
    "            if pd.Timestamp(find_next_market_date(date_point)) > pd.Timestamp(mask.columns[flag+1]):\n",
    "                flag+=1\n",
    "                c_mask = mask.iloc[:,flag]\n",
    "            date_point_marked = date_point\n",
    "            first = get_market_data_on_date(date_point_marked, print_error = False).loc[:,['종목코드', '종가', '시가총액', '등락률']]\n",
    "            date_point = find_next_market_date(date_point_marked)\n",
    "            second = get_market_data_on_date(date_point, print_error = False).loc[:,['종목코드', '종가', '시가총액', '등락률']]\n",
    "            date_list.append(date_point)\n",
    "            return_list = [[] for x in np.arange(mask.max()[0]+1)]\n",
    "            value_list = [[] for x in np.arange(mask.max()[0]+1)]\n",
    "            for i in range(len(first)):\n",
    "                if first.iloc[i,0] in c_mask.index and c_mask.notna().loc[first.iloc[i,0]]:\n",
    "                    if second.notna()[second['종목코드'] == first.iloc[i,0]].iloc[0,0]:\n",
    "                        return_list[int(c_mask.loc[first.iloc[i,0]])].append(second[second['종목코드'] == first.iloc[i,0]].iloc[0,3])\n",
    "                    else:\n",
    "                        return_list[int(c_mask.loc[first.iloc[i,0]])].append(-0.99)\n",
    "                    value_list[int(c_mask.loc[first.iloc[i,0]])].append(first.iloc[i,2])\n",
    "                    \n",
    "            if weight == 'VW':\n",
    "                for i in range(len(return_list)):\n",
    "                    # len을 곱해준 건 나중에 mean에서 EW와 같은 형태로 들어가게 하도록 하기 위함.\n",
    "                    return_list[i] *= value_list[i]/ np.sum(value_list[i]) * len(value_list[i])\n",
    "                    \n",
    "            # winsorize\n",
    "            if winsorize_limits > 0:\n",
    "                for i in range(len(return_list)):\n",
    "                    return_list[i] = winsorize(return_list[i], limits = winsorize_limits)\n",
    "                \n",
    "            factor = [np.mean(x) for x in return_list]\n",
    "            total_factor.append(factor)\n",
    "        return pd.DataFrame(total_factor, index = date_list)\n",
    "    elif term == 'm':\n",
    "        flag = -1\n",
    "        for _ in tqdm.tqdm(range(factor_months)):\n",
    "            if flag +1 == mask.shape[1]:\n",
    "                c_mask = mask.iloc[:,flag]\n",
    "                date_point_marked = date_point\n",
    "                first = (get_market_data_on_date(date_point_marked, print_error = False).loc[:,['종목코드', '시가', '시가총액', '등락률']]).set_index('종목코드')\n",
    "                c_mask = c_mask.loc[list(filter(lambda x: x in first.index, c_mask[c_mask.notna()].index))]\n",
    "            elif pd.Timestamp(find_next_market_date(date_point)) > pd.Timestamp(mask.columns[flag+1]):\n",
    "                flag+=1\n",
    "                c_mask = mask.iloc[:,flag]\n",
    "                date_point_marked = last_market_date(date_point)\n",
    "                first = (get_market_data_on_date(date_point_marked, print_error = False).loc[:,['종목코드', '시가', '시가총액', '등락률']]).set_index('종목코드')\n",
    "                c_mask = c_mask.loc[list(filter(lambda x: x in first.index, c_mask[c_mask.notna()].index))]\n",
    "            else:\n",
    "                date_point_marked = date_point\n",
    "                first = (get_market_data_on_date(date_point_marked, print_error = False).loc[:,['종목코드', '종가', '시가총액', '등락률']]).set_index('종목코드')\n",
    "            \n",
    "            # 통일된 수식으로 flag가 바뀐 경우의 말일을 계산할 수 있음\n",
    "            date_point = last_market_date(find_next_market_date(date_point_marked))\n",
    "            second = (get_market_data_on_date(date_point, print_error = False).loc[:,['종목코드', '종가', '시가총액', '등락률']]).set_index('종목코드')\n",
    "            date_list.append(date_point[:6])\n",
    "            return_value = get_market_return_for_period(date_point_marked, date_point)\n",
    "            return_list = []\n",
    "            value_list = []\n",
    "            for i in range(int(mask.max()[0]+1)):\n",
    "                return_list.append(return_value.loc[c_mask[c_mask == i].index].loc[:,['등락률']].values)\n",
    "                value_list.append(return_value.loc[c_mask[c_mask == i].index].loc[:,['시가총액']].values)\n",
    "            c_mask.drop(list(set(c_mask.index) - set(second.index)), inplace=True)\n",
    "            if weight == 'VW':\n",
    "                for i in range(len(return_list)):\n",
    "                    # len을 곱해준 건 나중에 mean에서 EW와 같은 형태로 들어가게 하도록 하기 위함.\n",
    "                    return_list[i] *= value_list[i]/ np.sum(value_list[i]) * len(value_list[i])\n",
    "            # winsorize\n",
    "            if winsorize_limits > 0:\n",
    "                for i in range(len(return_list)):\n",
    "                    return_list[i] = winsorize(return_list[i], limits = winsorize_limits)\n",
    "            factor = [np.mean(x) for x in return_list]\n",
    "            total_factor.append(factor)\n",
    "        return pd.DataFrame(total_factor, index = date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8706d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_portfolio(pfo, factor, market_bp= 'All', breakpoint=[0.5]):\n",
    "    # 여기서 앞에 들어가는 거는 마스크, 뒤에 들어가는 거는 실제값. \n",
    "    max_num = int(pfo.max()[0]+1)\n",
    "    new_df = pd.DataFrame(index = pfo.index, columns = pfo.columns)\n",
    "    for i in range(len(pfo.columns)):\n",
    "        for j in range(max_num):\n",
    "            get_index = list(filter(lambda x: x in factor.index, pfo.iloc[:,i][pfo.iloc[:,i]==j].index))\n",
    "            pfo_mask = get_mask(pd.DataFrame(factor.loc[get_index].iloc[:,i]), get_market_category(pd.DataFrame(factor.loc[get_index].iloc[:,i])), market_bp, breakpoint)\n",
    "            new_df.iloc[:,i].loc[pfo_mask.index] = pfo_mask.values[:,0]\n",
    "    return new_df\n",
    "\n",
    "def serialize_pfo(mask_list: list):\n",
    "    # 사전식 배열. 앞에서부터 사전식으로 serialize하여 뱉음\n",
    "    serial_num = [x.max().max()+1 for x in mask_list]\n",
    "    for i in reversed(range(len(serial_num)-1)):\n",
    "        serial_num[i] *= serial_num[i+1]\n",
    "    serial_num += [1]\n",
    "    serial_num = serial_num[1:]\n",
    "    serial_elem = [x * mask_list[i] for i, x in enumerate(serial_num)]\n",
    "    for i in range(len(serial_elem)-1):\n",
    "        serial_elem[0] += serial_elem[i+1]\n",
    "    return serial_elem[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31863169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_market_return_for_period(date_from, date_to):\n",
    "    first = get_market_data_on_date(find_next_market_date(date_from, today_opt = True), print_error = False).loc[:, ['종목코드', '종가', '시가총액']].set_index('종목코드')\n",
    "    df = pd.DataFrame()\n",
    "    flag = find_next_market_date(date_from, today_opt = True)\n",
    "    while flag != date_to:\n",
    "        flag = find_next_market_date(flag)\n",
    "        df = pd.concat([df, get_market_data_on_date(flag, print_error = False).loc[:,['종목코드', '등락률']].set_index('종목코드').rename({'등락률':flag}, axis=1)], axis=1)\n",
    "        if flag == date_to:\n",
    "            second = get_market_data_on_date(flag, print_error = False).loc[:, ['종목코드', '종가']].set_index('종목코드')\n",
    "    df = df.applymap(lambda x: (100 + x)/100)\n",
    "    for i in range(len(df.columns)):\n",
    "        if i == 0:\n",
    "            geo_sum = df.iloc[:,i].copy()\n",
    "        else:\n",
    "            geo_sum *= df.values[:,i]\n",
    "    compute_by_price = second['종가'] / first['종가']\n",
    "    # 가정 : 3% 이상 차이가 날 수 없다. 계산상 최대 1프로 정도 차이지만, 좀 갭을 두었음.\n",
    "    final_return = (compute_by_price - geo_sum < 0.03) * compute_by_price + (compute_by_price - geo_sum >= 0.03) * geo_sum\n",
    "    final_return = pd.concat([final_return.loc[first.index].fillna(0.01), first['시가총액']], axis=1).rename({0:'등락률'}, axis=1)\n",
    "    final_return['등락률'] = final_return['등락률'] -1\n",
    "    return final_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b38879ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_market_return(mask, term = 'd'):\n",
    "    term_conservative = pd.Timestamp(mask.columns[1]) - pd.Timestamp(mask.columns[0]) - pd.Timedelta(days = 7)\n",
    "    if pd.Timestamp(mask.columns[-1]) + term_conservative >  pd.Timestamp('20210630'):\n",
    "        date_to = pd.Timestamp('20210630')\n",
    "    else:\n",
    "        date_to = pd.Timestamp(last_market_date((pd.Timestamp(mask.columns[-1]) + term_conservative).strftime('%Y%m%d')))\n",
    "    date_from = pd.Timestamp(mask.columns[0])\n",
    "    factor_days = (date_to - date_from).days\n",
    "    factor_months = (date_to.year - date_from.year)*12 + (date_to.month - date_from.month)\n",
    "    \n",
    "    date_point = date_from.strftime('%Y%m%d')\n",
    "    # this is for storing the last date\n",
    "    date_point_marked = None\n",
    "    \n",
    "    total_return = []\n",
    "    date_list = []\n",
    "    \n",
    "    if term == 'd':\n",
    "        flag = -1\n",
    "        date_point = find_next_market_date(date_point)\n",
    "        for _ in tqdm.tqdm(range(factor_days)):\n",
    "            if pd.Timestamp(find_next_market_date(date_point)) > pd.Timestamp(mask.columns[flag+1]):\n",
    "                flag+=1\n",
    "                c_mask = mask.iloc[:,flag]\n",
    "            first = get_market_data_on_date(date_point, print_error = False).loc[:,['종목코드', '종가', '시가총액', '등락률']]\n",
    "            date_list.append(date_point)\n",
    "            date_point = find_next_market_date(date_point)\n",
    "            \n",
    "            market_cap = first['시가총액'].sum()\n",
    "            R = first['등락률'] * (first['시가총액'] / market_cap)\n",
    "            total_return.append(R.fillna(0).sum())\n",
    "            \n",
    "        return pd.DataFrame(total_return, index = date_list)\n",
    "    elif term == 'm':\n",
    "        flag = -1\n",
    "        for _ in tqdm.tqdm(range(factor_months)):\n",
    "            if flag +1 == mask.shape[1]:\n",
    "                c_mask = mask.iloc[:,flag]\n",
    "                date_point = find_next_market_date(date_point)\n",
    "                date_point_marked = date_point\n",
    "            elif pd.Timestamp(find_next_market_date(date_point)) > pd.Timestamp(mask.columns[flag+1]):\n",
    "                flag+=1\n",
    "                c_mask = mask.iloc[:,flag]\n",
    "                date_point_marked = last_market_date(date_point)\n",
    "            else:\n",
    "                date_point_marked = date_point\n",
    "            \n",
    "            date_point = last_market_date(find_next_market_date(date_point_marked))\n",
    "            return_value = get_market_return_for_period(date_point_marked, date_point)\n",
    "            date_list.append(date_point[:6])\n",
    "            market_cap = return_value['시가총액'].sum()\n",
    "            R = return_value['등락률'] * (return_value['시가총액'] / market_cap)\n",
    "            \n",
    "            total_return.append(R.replace([np.inf, -np.inf], np.nan).fillna(0).sum())\n",
    "        return pd.DataFrame(total_return, index = date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b789d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_market_return_for_period2(date_from, date_to):\n",
    "    return total_mr[mr_date.index(date_to)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4604dacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_factor_on_date_by_mask2(mask, term = 'd', winsorize_limits = 0.01, weight = 'EW'):\n",
    "    # @@@@@@@@@@@ option 추가해야할듯! : daily / weekly / monthly / quarterly 정도까지는 구현을 해놓아야할듯\n",
    "    # monthly 계산할 때 첫 달은 어떻게 계산하는거지? 전달 말일 기준으로 계산하나? 아니면 해당 달 시초가 기준으로 계산하나?\n",
    "    # -> 우선 첫 달은 시가 기준, 이후로는 전달 말일에 리밸런싱 하는 걸 기준으로 계산했음. \n",
    "    term_conservative = pd.Timestamp(mask.columns[1]) - pd.Timestamp(mask.columns[0]) - pd.Timedelta(days = 7)\n",
    "    if pd.Timestamp(mask.columns[-1]) + term_conservative >  pd.Timestamp('20210630'):\n",
    "        date_to = pd.Timestamp('20210630')\n",
    "    else:\n",
    "        date_to = pd.Timestamp(last_market_date((pd.Timestamp(mask.columns[-1]) + term_conservative).strftime('%Y%m%d')))\n",
    "    date_from = pd.Timestamp(mask.columns[0])\n",
    "    factor_days = (date_to - date_from).days\n",
    "    factor_months = (date_to.year - date_from.year)*12 + (date_to.month - date_from.month)\n",
    "    \n",
    "    date_point = date_from.strftime('%Y%m%d')\n",
    "    # this is for storing the last date\n",
    "    date_point_marked = None\n",
    "    \n",
    "    total_factor = []\n",
    "    date_list = []\n",
    "    \n",
    "    if term == 'd':\n",
    "        flag = -1\n",
    "#         for _ in range(factor_days):\n",
    "        for _ in tqdm.tqdm(range(10)):\n",
    "            if pd.Timestamp(find_next_market_date(date_point)) > pd.Timestamp(mask.columns[flag+1]):\n",
    "                flag+=1\n",
    "                c_mask = mask.iloc[:,flag]\n",
    "            date_point_marked = date_point\n",
    "            first = get_market_data_on_date(date_point_marked, print_error = False).loc[:,['종목코드', '종가', '시가총액', '등락률']]\n",
    "            date_point = find_next_market_date(date_point_marked)\n",
    "            second = get_market_data_on_date(date_point, print_error = False).loc[:,['종목코드', '종가', '시가총액', '등락률']]\n",
    "            date_list.append(date_point)\n",
    "            return_list = [[] for x in np.arange(mask.max()[0]+1)]\n",
    "            value_list = [[] for x in np.arange(mask.max()[0]+1)]\n",
    "            for i in range(len(first)):\n",
    "                if first.iloc[i,0] in c_mask.index and c_mask.notna().loc[first.iloc[i,0]]:\n",
    "                    if second.notna()[second['종목코드'] == first.iloc[i,0]].iloc[0,0]:\n",
    "                        return_list[int(c_mask.loc[first.iloc[i,0]])].append(second[second['종목코드'] == first.iloc[i,0]].iloc[0,3])\n",
    "                    else:\n",
    "                        return_list[int(c_mask.loc[first.iloc[i,0]])].append(-0.99)\n",
    "                    value_list[int(c_mask.loc[first.iloc[i,0]])].append(first.iloc[i,2])\n",
    "                    \n",
    "            if weight == 'VW':\n",
    "                for i in range(len(return_list)):\n",
    "                    # len을 곱해준 건 나중에 mean에서 EW와 같은 형태로 들어가게 하도록 하기 위함.\n",
    "                    return_list[i] *= value_list[i]/ np.sum(value_list[i]) * len(value_list[i])\n",
    "                    \n",
    "            # winsorize\n",
    "            if winsorize_limits > 0:\n",
    "                for i in range(len(return_list)):\n",
    "                    return_list[i] = winsorize(return_list[i], limits = winsorize_limits)\n",
    "                \n",
    "            factor = [np.mean(x) for x in return_list]\n",
    "            total_factor.append(factor)\n",
    "        return pd.DataFrame(total_factor, index = date_list)\n",
    "    elif term == 'm':\n",
    "        flag = -1\n",
    "        for _ in tqdm.tqdm(range(factor_months)):\n",
    "            if flag +1 == mask.shape[1]:\n",
    "                c_mask = mask.iloc[:,flag]\n",
    "                date_point_marked = date_point\n",
    "                first = (get_market_data_on_date(date_point_marked, print_error = False).loc[:,['종목코드', '시가', '시가총액', '등락률']]).set_index('종목코드')\n",
    "                c_mask = c_mask.loc[list(filter(lambda x: x in first.index, c_mask[c_mask.notna()].index))]\n",
    "            elif pd.Timestamp(find_next_market_date(date_point)) > pd.Timestamp(mask.columns[flag+1]):\n",
    "                flag+=1\n",
    "                c_mask = mask.iloc[:,flag]\n",
    "                date_point_marked = last_market_date(date_point)\n",
    "                first = (get_market_data_on_date(date_point_marked, print_error = False).loc[:,['종목코드', '시가', '시가총액', '등락률']]).set_index('종목코드')\n",
    "                c_mask = c_mask.loc[list(filter(lambda x: x in first.index, c_mask[c_mask.notna()].index))]\n",
    "            else:\n",
    "                date_point_marked = date_point\n",
    "                first = (get_market_data_on_date(date_point_marked, print_error = False).loc[:,['종목코드', '종가', '시가총액', '등락률']]).set_index('종목코드')\n",
    "            \n",
    "            # 통일된 수식으로 flag가 바뀐 경우의 말일을 계산할 수 있음\n",
    "            date_point = last_market_date(find_next_market_date(date_point_marked))\n",
    "            second = (get_market_data_on_date(date_point, print_error = False).loc[:,['종목코드', '종가', '시가총액', '등락률']]).set_index('종목코드')\n",
    "            date_list.append(date_point[:6])\n",
    "            return_value = get_market_return_for_period2(date_point_marked, date_point)\n",
    "            return_list = []\n",
    "            value_list = []\n",
    "            for i in range(int(mask.max()[0]+1)):\n",
    "                return_list.append(return_value.loc[c_mask[c_mask == i].index].loc[:,['등락률']].values)\n",
    "                value_list.append(return_value.loc[c_mask[c_mask == i].index].loc[:,['시가총액']].values)\n",
    "            c_mask.drop(list(set(c_mask.index) - set(second.index)), inplace=True)\n",
    "            if weight == 'VW':\n",
    "                for i in range(len(return_list)):\n",
    "                    # len을 곱해준 건 나중에 mean에서 EW와 같은 형태로 들어가게 하도록 하기 위함.\n",
    "                    return_list[i] *= value_list[i]/ np.sum(value_list[i]) * len(value_list[i])\n",
    "            # winsorize\n",
    "            if winsorize_limits > 0:\n",
    "                for i in range(len(return_list)):\n",
    "                    return_list[i] = winsorize(return_list[i], limits = winsorize_limits)\n",
    "            factor = [np.mean(x) for x in return_list]\n",
    "            total_factor.append(factor)\n",
    "        return pd.DataFrame(total_factor, index = date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50b593c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_risk_free_rate(pfo, risk_free):\n",
    "    return risk_free.loc[pfo.index]\n",
    "\n",
    "def get_ols_model(mask, R_m, pfo_column, risk_free, *args):\n",
    "    R_f = (get_risk_free_rate(pfo_column, risk_free).astype(float)/1200).squeeze() #monthly로 바꾸고 %를 소수로 바꿔야하므로 1200으로 나눔\n",
    "    df = pd.DataFrame([(pfo_column - R_f.T).values.squeeze(), (R_m - R_f).values.squeeze()] + [x.values.squeeze() for x in args]).T\n",
    "    df.columns = ['pfo', 'market'] + ['factor' + str(i) for i, x in enumerate(args)]\n",
    "    var_str = ''\n",
    "    for i in range(len(df.columns)-2):\n",
    "        var_str += ' + ' + ['factor' + str(i) for i, x in enumerate(args)][i]\n",
    "    model = ols(formula = 'pfo ~ market' + var_str,data = df).fit()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b29ab051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# risk free rate\n",
    "os.chdir('C:\\\\Users\\\\jky93\\\\KYdrive\\\\바탕 화면\\\\창업\\\\팩터프로젝트')\n",
    "cd_91 = pd.read_csv('cd금리.csv', encoding='cp949', header=2, index_col=0).T.iloc[:, [4]]\n",
    "cd_91.index = cd_91.index.map(lambda x: x[:6])\n",
    "cd_91.columns = ['cd_91']\n",
    "os.chdir('C:\\\\Users\\\\jky93\\\\KYdrive\\\\바탕 화면\\\\창업\\\\팩터프로젝트\\\\KRX_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd6e59d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf0e7876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00_10_10.csv',\n",
       " '00_10_11.csv',\n",
       " '00_10_12.csv',\n",
       " '00_10_13.csv',\n",
       " '00_10_16.csv',\n",
       " '00_10_17.csv',\n",
       " '00_10_18.csv',\n",
       " '00_10_19.csv',\n",
       " '00_10_2.csv',\n",
       " '00_10_20.csv',\n",
       " '00_10_23.csv',\n",
       " '00_10_24.csv',\n",
       " '00_10_25.csv',\n",
       " '00_10_26.csv',\n",
       " '00_10_27.csv',\n",
       " '00_10_3.csv',\n",
       " '00_10_30.csv',\n",
       " '00_10_31.csv',\n",
       " '00_10_4.csv',\n",
       " '00_10_5.csv',\n",
       " '00_10_6.csv',\n",
       " '00_10_9.csv',\n",
       " '00_11_1.csv',\n",
       " '00_11_10.csv',\n",
       " '00_11_13.csv',\n",
       " '00_11_14.csv',\n",
       " '00_11_15.csv',\n",
       " '00_11_16.csv',\n",
       " '00_11_17.csv',\n",
       " '00_11_2.csv',\n",
       " '00_11_20.csv',\n",
       " '00_11_21.csv',\n",
       " '00_11_22.csv',\n",
       " '00_11_23.csv',\n",
       " '00_11_24.csv',\n",
       " '00_11_27.csv',\n",
       " '00_11_28.csv',\n",
       " '00_11_29.csv',\n",
       " '00_11_3.csv',\n",
       " '00_11_30.csv',\n",
       " '00_11_6.csv',\n",
       " '00_11_7.csv',\n",
       " '00_11_8.csv',\n",
       " '00_11_9.csv',\n",
       " '00_12_1.csv',\n",
       " '00_12_11.csv',\n",
       " '00_12_12.csv',\n",
       " '00_12_13.csv',\n",
       " '00_12_14.csv',\n",
       " '00_12_15.csv',\n",
       " '00_12_18.csv',\n",
       " '00_12_19.csv',\n",
       " '00_12_20.csv',\n",
       " '00_12_21.csv',\n",
       " '00_12_22.csv',\n",
       " '00_12_25.csv',\n",
       " '00_12_26.csv',\n",
       " '00_12_4.csv',\n",
       " '00_12_5.csv',\n",
       " '00_12_6.csv',\n",
       " '00_12_7.csv',\n",
       " '00_12_8.csv',\n",
       " '00_1_10.csv',\n",
       " '00_1_11.csv',\n",
       " '00_1_12.csv',\n",
       " '00_1_13.csv',\n",
       " '00_1_14.csv',\n",
       " '00_1_17.csv',\n",
       " '00_1_18.csv',\n",
       " '00_1_19.csv',\n",
       " '00_1_20.csv',\n",
       " '00_1_21.csv',\n",
       " '00_1_24.csv',\n",
       " '00_1_25.csv',\n",
       " '00_1_26.csv',\n",
       " '00_1_27.csv',\n",
       " '00_1_28.csv',\n",
       " '00_1_31.csv',\n",
       " '00_1_4.csv',\n",
       " '00_1_5.csv',\n",
       " '00_1_6.csv',\n",
       " '00_1_7.csv',\n",
       " '00_2_1.csv',\n",
       " '00_2_10.csv',\n",
       " '00_2_11.csv',\n",
       " '00_2_14.csv',\n",
       " '00_2_15.csv',\n",
       " '00_2_16.csv',\n",
       " '00_2_17.csv',\n",
       " '00_2_18.csv',\n",
       " '00_2_2.csv',\n",
       " '00_2_21.csv',\n",
       " '00_2_22.csv',\n",
       " '00_2_23.csv',\n",
       " '00_2_24.csv',\n",
       " '00_2_25.csv',\n",
       " '00_2_28.csv',\n",
       " '00_2_29.csv',\n",
       " '00_2_3.csv',\n",
       " '00_2_4.csv',\n",
       " '00_2_7.csv',\n",
       " '00_2_8.csv',\n",
       " '00_2_9.csv',\n",
       " '00_3_1.csv',\n",
       " '00_3_10.csv',\n",
       " '00_3_13.csv',\n",
       " '00_3_14.csv',\n",
       " '00_3_15.csv',\n",
       " '00_3_16.csv',\n",
       " '00_3_17.csv',\n",
       " '00_3_2.csv',\n",
       " '00_3_20.csv',\n",
       " '00_3_21.csv',\n",
       " '00_3_22.csv',\n",
       " '00_3_23.csv',\n",
       " '00_3_24.csv',\n",
       " '00_3_27.csv',\n",
       " '00_3_28.csv',\n",
       " '00_3_29.csv',\n",
       " '00_3_3.csv',\n",
       " '00_3_30.csv',\n",
       " '00_3_31.csv',\n",
       " '00_3_6.csv',\n",
       " '00_3_7.csv',\n",
       " '00_3_8.csv',\n",
       " '00_3_9.csv',\n",
       " '00_4_10.csv',\n",
       " '00_4_11.csv',\n",
       " '00_4_12.csv',\n",
       " '00_4_13.csv',\n",
       " '00_4_14.csv',\n",
       " '00_4_17.csv',\n",
       " '00_4_18.csv',\n",
       " '00_4_19.csv',\n",
       " '00_4_20.csv',\n",
       " '00_4_21.csv',\n",
       " '00_4_24.csv',\n",
       " '00_4_25.csv',\n",
       " '00_4_26.csv',\n",
       " '00_4_27.csv',\n",
       " '00_4_28.csv',\n",
       " '00_4_3.csv',\n",
       " '00_4_4.csv',\n",
       " '00_4_5.csv',\n",
       " '00_4_6.csv',\n",
       " '00_4_7.csv',\n",
       " '00_5_1.csv',\n",
       " '00_5_10.csv',\n",
       " '00_5_11.csv',\n",
       " '00_5_12.csv',\n",
       " '00_5_15.csv',\n",
       " '00_5_16.csv',\n",
       " '00_5_17.csv',\n",
       " '00_5_18.csv',\n",
       " '00_5_19.csv',\n",
       " '00_5_2.csv',\n",
       " '00_5_22.csv',\n",
       " '00_5_23.csv',\n",
       " '00_5_24.csv',\n",
       " '00_5_25.csv',\n",
       " '00_5_26.csv',\n",
       " '00_5_29.csv',\n",
       " '00_5_3.csv',\n",
       " '00_5_30.csv',\n",
       " '00_5_31.csv',\n",
       " '00_5_4.csv',\n",
       " '00_5_5.csv',\n",
       " '00_5_8.csv',\n",
       " '00_5_9.csv',\n",
       " '00_6_1.csv',\n",
       " '00_6_12.csv',\n",
       " '00_6_13.csv',\n",
       " '00_6_14.csv',\n",
       " '00_6_15.csv',\n",
       " '00_6_16.csv',\n",
       " '00_6_19.csv',\n",
       " '00_6_2.csv',\n",
       " '00_6_20.csv',\n",
       " '00_6_21.csv',\n",
       " '00_6_22.csv',\n",
       " '00_6_23.csv',\n",
       " '00_6_26.csv',\n",
       " '00_6_27.csv',\n",
       " '00_6_28.csv',\n",
       " '00_6_29.csv',\n",
       " '00_6_30.csv',\n",
       " '00_6_5.csv',\n",
       " '00_6_6.csv',\n",
       " '00_6_7.csv',\n",
       " '00_6_8.csv',\n",
       " '00_6_9.csv',\n",
       " '00_7_10.csv',\n",
       " '00_7_11.csv',\n",
       " '00_7_12.csv',\n",
       " '00_7_13.csv',\n",
       " '00_7_14.csv',\n",
       " '00_7_17.csv',\n",
       " '00_7_18.csv',\n",
       " '00_7_19.csv',\n",
       " '00_7_20.csv',\n",
       " '00_7_21.csv',\n",
       " '00_7_24.csv',\n",
       " '00_7_25.csv',\n",
       " '00_7_26.csv',\n",
       " '00_7_27.csv',\n",
       " '00_7_28.csv',\n",
       " '00_7_3.csv',\n",
       " '00_7_31.csv',\n",
       " '00_7_4.csv',\n",
       " '00_7_5.csv',\n",
       " '00_7_6.csv',\n",
       " '00_7_7.csv',\n",
       " '00_8_1.csv',\n",
       " '00_8_10.csv',\n",
       " '00_8_11.csv',\n",
       " '00_8_14.csv',\n",
       " '00_8_15.csv',\n",
       " '00_8_16.csv',\n",
       " '00_8_17.csv',\n",
       " '00_8_18.csv',\n",
       " '00_8_2.csv',\n",
       " '00_8_21.csv',\n",
       " '00_8_22.csv',\n",
       " '00_8_23.csv',\n",
       " '00_8_24.csv',\n",
       " '00_8_25.csv',\n",
       " '00_8_28.csv',\n",
       " '00_8_29.csv',\n",
       " '00_8_3.csv',\n",
       " '00_8_30.csv',\n",
       " '00_8_31.csv',\n",
       " '00_8_4.csv',\n",
       " '00_8_7.csv',\n",
       " '00_8_8.csv',\n",
       " '00_8_9.csv',\n",
       " '00_9_1.csv',\n",
       " '00_9_11.csv',\n",
       " '00_9_12.csv',\n",
       " '00_9_13.csv',\n",
       " '00_9_14.csv',\n",
       " '00_9_15.csv',\n",
       " '00_9_18.csv',\n",
       " '00_9_19.csv',\n",
       " '00_9_20.csv',\n",
       " '00_9_21.csv',\n",
       " '00_9_22.csv',\n",
       " '00_9_25.csv',\n",
       " '00_9_26.csv',\n",
       " '00_9_27.csv',\n",
       " '00_9_28.csv',\n",
       " '00_9_29.csv',\n",
       " '00_9_4.csv',\n",
       " '00_9_5.csv',\n",
       " '00_9_6.csv',\n",
       " '00_9_7.csv',\n",
       " '00_9_8.csv',\n",
       " '01_10_1.csv',\n",
       " '01_10_10.csv',\n",
       " '01_10_11.csv',\n",
       " '01_10_12.csv',\n",
       " '01_10_15.csv',\n",
       " '01_10_16.csv',\n",
       " '01_10_17.csv',\n",
       " '01_10_18.csv',\n",
       " '01_10_19.csv',\n",
       " '01_10_2.csv',\n",
       " '01_10_22.csv',\n",
       " '01_10_23.csv',\n",
       " '01_10_24.csv',\n",
       " '01_10_25.csv',\n",
       " '01_10_26.csv',\n",
       " '01_10_29.csv',\n",
       " '01_10_3.csv',\n",
       " '01_10_30.csv',\n",
       " '01_10_31.csv',\n",
       " '01_10_4.csv',\n",
       " '01_10_5.csv',\n",
       " '01_10_8.csv',\n",
       " '01_10_9.csv',\n",
       " '01_11_1.csv',\n",
       " '01_11_12.csv',\n",
       " '01_11_13.csv',\n",
       " '01_11_14.csv',\n",
       " '01_11_15.csv',\n",
       " '01_11_16.csv',\n",
       " '01_11_19.csv',\n",
       " '01_11_2.csv',\n",
       " '01_11_20.csv',\n",
       " '01_11_21.csv',\n",
       " '01_11_22.csv',\n",
       " '01_11_23.csv',\n",
       " '01_11_26.csv',\n",
       " '01_11_27.csv',\n",
       " '01_11_28.csv',\n",
       " '01_11_29.csv',\n",
       " '01_11_30.csv',\n",
       " '01_11_5.csv',\n",
       " '01_11_6.csv',\n",
       " '01_11_7.csv',\n",
       " '01_11_8.csv',\n",
       " '01_11_9.csv',\n",
       " '01_12_10.csv',\n",
       " '01_12_11.csv',\n",
       " '01_12_12.csv',\n",
       " '01_12_13.csv',\n",
       " '01_12_14.csv',\n",
       " '01_12_17.csv',\n",
       " '01_12_18.csv',\n",
       " '01_12_19.csv',\n",
       " '01_12_20.csv',\n",
       " '01_12_21.csv',\n",
       " '01_12_24.csv',\n",
       " '01_12_25.csv',\n",
       " '01_12_26.csv',\n",
       " '01_12_27.csv',\n",
       " '01_12_28.csv',\n",
       " '01_12_3.csv',\n",
       " '01_12_4.csv',\n",
       " '01_12_5.csv',\n",
       " '01_12_6.csv',\n",
       " '01_12_7.csv',\n",
       " '01_1_10.csv',\n",
       " '01_1_11.csv',\n",
       " '01_1_12.csv',\n",
       " '01_1_15.csv',\n",
       " '01_1_16.csv',\n",
       " '01_1_17.csv',\n",
       " '01_1_18.csv',\n",
       " '01_1_19.csv',\n",
       " '01_1_2.csv',\n",
       " '01_1_22.csv',\n",
       " '01_1_23.csv',\n",
       " '01_1_24.csv',\n",
       " '01_1_25.csv',\n",
       " '01_1_26.csv',\n",
       " '01_1_29.csv',\n",
       " '01_1_3.csv',\n",
       " '01_1_30.csv',\n",
       " '01_1_31.csv',\n",
       " '01_1_4.csv',\n",
       " '01_1_5.csv',\n",
       " '01_1_8.csv',\n",
       " '01_1_9.csv',\n",
       " '01_2_1.csv',\n",
       " '01_2_12.csv',\n",
       " '01_2_13.csv',\n",
       " '01_2_14.csv',\n",
       " '01_2_15.csv',\n",
       " '01_2_16.csv',\n",
       " '01_2_19.csv',\n",
       " '01_2_2.csv',\n",
       " '01_2_20.csv',\n",
       " '01_2_21.csv',\n",
       " '01_2_22.csv',\n",
       " '01_2_23.csv',\n",
       " '01_2_26.csv',\n",
       " '01_2_27.csv',\n",
       " '01_2_28.csv',\n",
       " '01_2_5.csv',\n",
       " '01_2_6.csv',\n",
       " '01_2_7.csv',\n",
       " '01_2_8.csv',\n",
       " '01_2_9.csv',\n",
       " '01_3_1.csv',\n",
       " '01_3_12.csv',\n",
       " '01_3_13.csv',\n",
       " '01_3_14.csv',\n",
       " '01_3_15.csv',\n",
       " '01_3_16.csv',\n",
       " '01_3_19.csv',\n",
       " '01_3_2.csv',\n",
       " '01_3_20.csv',\n",
       " '01_3_21.csv',\n",
       " '01_3_22.csv',\n",
       " '01_3_23.csv',\n",
       " '01_3_26.csv',\n",
       " '01_3_27.csv',\n",
       " '01_3_28.csv',\n",
       " '01_3_29.csv',\n",
       " '01_3_30.csv',\n",
       " '01_3_5.csv',\n",
       " '01_3_6.csv',\n",
       " '01_3_7.csv',\n",
       " '01_3_8.csv',\n",
       " '01_3_9.csv',\n",
       " '01_4_10.csv',\n",
       " '01_4_11.csv',\n",
       " '01_4_12.csv',\n",
       " '01_4_13.csv',\n",
       " '01_4_16.csv',\n",
       " '01_4_17.csv',\n",
       " '01_4_18.csv',\n",
       " '01_4_19.csv',\n",
       " '01_4_2.csv',\n",
       " '01_4_20.csv',\n",
       " '01_4_23.csv',\n",
       " '01_4_24.csv',\n",
       " '01_4_25.csv',\n",
       " '01_4_26.csv',\n",
       " '01_4_27.csv',\n",
       " '01_4_3.csv',\n",
       " '01_4_30.csv',\n",
       " '01_4_4.csv',\n",
       " '01_4_5.csv',\n",
       " '01_4_6.csv',\n",
       " '01_4_9.csv',\n",
       " '01_5_1.csv',\n",
       " '01_5_10.csv',\n",
       " '01_5_11.csv',\n",
       " '01_5_14.csv',\n",
       " '01_5_15.csv',\n",
       " '01_5_16.csv',\n",
       " '01_5_17.csv',\n",
       " '01_5_18.csv',\n",
       " '01_5_2.csv',\n",
       " '01_5_21.csv',\n",
       " '01_5_22.csv',\n",
       " '01_5_23.csv',\n",
       " '01_5_24.csv',\n",
       " '01_5_25.csv',\n",
       " '01_5_28.csv',\n",
       " '01_5_29.csv',\n",
       " '01_5_3.csv',\n",
       " '01_5_30.csv',\n",
       " '01_5_31.csv',\n",
       " '01_5_4.csv',\n",
       " '01_5_7.csv',\n",
       " '01_5_8.csv',\n",
       " '01_5_9.csv',\n",
       " '01_6_1.csv',\n",
       " '01_6_11.csv',\n",
       " '01_6_12.csv',\n",
       " '01_6_13.csv',\n",
       " '01_6_14.csv',\n",
       " '01_6_15.csv',\n",
       " '01_6_18.csv',\n",
       " '01_6_19.csv',\n",
       " '01_6_20.csv',\n",
       " '01_6_21.csv',\n",
       " '01_6_22.csv',\n",
       " '01_6_25.csv',\n",
       " '01_6_26.csv',\n",
       " '01_6_27.csv',\n",
       " '01_6_28.csv',\n",
       " '01_6_29.csv',\n",
       " '01_6_4.csv',\n",
       " '01_6_5.csv',\n",
       " '01_6_6.csv',\n",
       " '01_6_7.csv',\n",
       " '01_6_8.csv',\n",
       " '01_7_10.csv',\n",
       " '01_7_11.csv',\n",
       " '01_7_12.csv',\n",
       " '01_7_13.csv',\n",
       " '01_7_16.csv',\n",
       " '01_7_17.csv',\n",
       " '01_7_18.csv',\n",
       " '01_7_19.csv',\n",
       " '01_7_2.csv',\n",
       " '01_7_20.csv',\n",
       " '01_7_23.csv',\n",
       " '01_7_24.csv',\n",
       " '01_7_25.csv',\n",
       " '01_7_26.csv',\n",
       " '01_7_27.csv',\n",
       " '01_7_3.csv',\n",
       " '01_7_30.csv',\n",
       " '01_7_31.csv',\n",
       " '01_7_4.csv',\n",
       " '01_7_5.csv',\n",
       " '01_7_6.csv',\n",
       " '01_7_9.csv',\n",
       " '01_8_1.csv',\n",
       " '01_8_10.csv',\n",
       " '01_8_13.csv',\n",
       " '01_8_14.csv',\n",
       " '01_8_15.csv',\n",
       " '01_8_16.csv',\n",
       " '01_8_17.csv',\n",
       " '01_8_2.csv',\n",
       " '01_8_20.csv',\n",
       " '01_8_21.csv',\n",
       " '01_8_22.csv',\n",
       " '01_8_23.csv',\n",
       " '01_8_24.csv',\n",
       " '01_8_27.csv',\n",
       " '01_8_28.csv',\n",
       " '01_8_29.csv',\n",
       " '01_8_3.csv',\n",
       " '01_8_30.csv',\n",
       " '01_8_31.csv',\n",
       " '01_8_6.csv',\n",
       " '01_8_7.csv',\n",
       " '01_8_8.csv',\n",
       " '01_8_9.csv',\n",
       " '01_9_10.csv',\n",
       " '01_9_11.csv',\n",
       " '01_9_12.csv',\n",
       " '01_9_13.csv',\n",
       " '01_9_14.csv',\n",
       " '01_9_17.csv',\n",
       " '01_9_18.csv',\n",
       " '01_9_19.csv',\n",
       " '01_9_20.csv',\n",
       " '01_9_21.csv',\n",
       " '01_9_24.csv',\n",
       " '01_9_25.csv',\n",
       " '01_9_26.csv',\n",
       " '01_9_27.csv',\n",
       " '01_9_28.csv',\n",
       " '01_9_3.csv',\n",
       " '01_9_4.csv',\n",
       " '01_9_5.csv',\n",
       " '01_9_6.csv',\n",
       " '01_9_7.csv',\n",
       " '02_10_1.csv',\n",
       " '02_10_10.csv',\n",
       " '02_10_11.csv',\n",
       " '02_10_14.csv',\n",
       " '02_10_15.csv',\n",
       " '02_10_16.csv',\n",
       " '02_10_17.csv',\n",
       " '02_10_18.csv',\n",
       " '02_10_2.csv',\n",
       " '02_10_21.csv',\n",
       " '02_10_22.csv',\n",
       " '02_10_23.csv',\n",
       " '02_10_24.csv',\n",
       " '02_10_25.csv',\n",
       " '02_10_28.csv',\n",
       " '02_10_29.csv',\n",
       " '02_10_3.csv',\n",
       " '02_10_30.csv',\n",
       " '02_10_31.csv',\n",
       " '02_10_4.csv',\n",
       " '02_10_7.csv',\n",
       " '02_10_8.csv',\n",
       " '02_10_9.csv',\n",
       " '02_11_1.csv',\n",
       " '02_11_11.csv',\n",
       " '02_11_12.csv',\n",
       " '02_11_13.csv',\n",
       " '02_11_14.csv',\n",
       " '02_11_15.csv',\n",
       " '02_11_18.csv',\n",
       " '02_11_19.csv',\n",
       " '02_11_20.csv',\n",
       " '02_11_21.csv',\n",
       " '02_11_22.csv',\n",
       " '02_11_25.csv',\n",
       " '02_11_26.csv',\n",
       " '02_11_27.csv',\n",
       " '02_11_28.csv',\n",
       " '02_11_29.csv',\n",
       " '02_11_4.csv',\n",
       " '02_11_5.csv',\n",
       " '02_11_6.csv',\n",
       " '02_11_7.csv',\n",
       " '02_11_8.csv',\n",
       " '02_12_10.csv',\n",
       " '02_12_11.csv',\n",
       " '02_12_12.csv',\n",
       " '02_12_13.csv',\n",
       " '02_12_16.csv',\n",
       " '02_12_17.csv',\n",
       " '02_12_18.csv',\n",
       " '02_12_19.csv',\n",
       " '02_12_2.csv',\n",
       " '02_12_20.csv',\n",
       " '02_12_23.csv',\n",
       " '02_12_24.csv',\n",
       " '02_12_25.csv',\n",
       " '02_12_26.csv',\n",
       " '02_12_27.csv',\n",
       " '02_12_3.csv',\n",
       " '02_12_30.csv',\n",
       " '02_12_4.csv',\n",
       " '02_12_5.csv',\n",
       " '02_12_6.csv',\n",
       " '02_12_9.csv',\n",
       " '02_1_10.csv',\n",
       " '02_1_11.csv',\n",
       " '02_1_14.csv',\n",
       " '02_1_15.csv',\n",
       " '02_1_16.csv',\n",
       " '02_1_17.csv',\n",
       " '02_1_18.csv',\n",
       " '02_1_2.csv',\n",
       " '02_1_21.csv',\n",
       " '02_1_22.csv',\n",
       " '02_1_23.csv',\n",
       " '02_1_24.csv',\n",
       " '02_1_25.csv',\n",
       " '02_1_28.csv',\n",
       " '02_1_29.csv',\n",
       " '02_1_3.csv',\n",
       " '02_1_30.csv',\n",
       " '02_1_31.csv',\n",
       " '02_1_4.csv',\n",
       " '02_1_7.csv',\n",
       " '02_1_8.csv',\n",
       " '02_1_9.csv',\n",
       " '02_2_1.csv',\n",
       " '02_2_11.csv',\n",
       " '02_2_12.csv',\n",
       " '02_2_13.csv',\n",
       " '02_2_14.csv',\n",
       " '02_2_15.csv',\n",
       " '02_2_18.csv',\n",
       " '02_2_19.csv',\n",
       " '02_2_20.csv',\n",
       " '02_2_21.csv',\n",
       " '02_2_22.csv',\n",
       " '02_2_25.csv',\n",
       " '02_2_26.csv',\n",
       " '02_2_27.csv',\n",
       " '02_2_28.csv',\n",
       " '02_2_4.csv',\n",
       " '02_2_5.csv',\n",
       " '02_2_6.csv',\n",
       " '02_2_7.csv',\n",
       " '02_2_8.csv',\n",
       " '02_3_1.csv',\n",
       " '02_3_11.csv',\n",
       " '02_3_12.csv',\n",
       " '02_3_13.csv',\n",
       " '02_3_14.csv',\n",
       " '02_3_15.csv',\n",
       " '02_3_18.csv',\n",
       " '02_3_19.csv',\n",
       " '02_3_20.csv',\n",
       " '02_3_21.csv',\n",
       " '02_3_22.csv',\n",
       " '02_3_25.csv',\n",
       " '02_3_26.csv',\n",
       " '02_3_27.csv',\n",
       " '02_3_28.csv',\n",
       " '02_3_29.csv',\n",
       " '02_3_4.csv',\n",
       " '02_3_5.csv',\n",
       " '02_3_6.csv',\n",
       " '02_3_7.csv',\n",
       " '02_3_8.csv',\n",
       " '02_4_1.csv',\n",
       " '02_4_10.csv',\n",
       " '02_4_11.csv',\n",
       " '02_4_12.csv',\n",
       " '02_4_15.csv',\n",
       " '02_4_16.csv',\n",
       " '02_4_17.csv',\n",
       " '02_4_18.csv',\n",
       " '02_4_19.csv',\n",
       " '02_4_2.csv',\n",
       " '02_4_22.csv',\n",
       " '02_4_23.csv',\n",
       " '02_4_24.csv',\n",
       " '02_4_25.csv',\n",
       " '02_4_26.csv',\n",
       " '02_4_29.csv',\n",
       " '02_4_3.csv',\n",
       " '02_4_30.csv',\n",
       " '02_4_4.csv',\n",
       " '02_4_5.csv',\n",
       " '02_4_8.csv',\n",
       " '02_4_9.csv',\n",
       " '02_5_1.csv',\n",
       " '02_5_10.csv',\n",
       " '02_5_13.csv',\n",
       " '02_5_14.csv',\n",
       " '02_5_15.csv',\n",
       " '02_5_16.csv',\n",
       " '02_5_17.csv',\n",
       " '02_5_2.csv',\n",
       " '02_5_20.csv',\n",
       " '02_5_21.csv',\n",
       " '02_5_22.csv',\n",
       " '02_5_23.csv',\n",
       " '02_5_24.csv',\n",
       " '02_5_27.csv',\n",
       " '02_5_28.csv',\n",
       " '02_5_29.csv',\n",
       " '02_5_3.csv',\n",
       " '02_5_30.csv',\n",
       " '02_5_31.csv',\n",
       " '02_5_6.csv',\n",
       " '02_5_7.csv',\n",
       " '02_5_8.csv',\n",
       " '02_5_9.csv',\n",
       " '02_6_10.csv',\n",
       " '02_6_11.csv',\n",
       " '02_6_12.csv',\n",
       " '02_6_13.csv',\n",
       " '02_6_14.csv',\n",
       " '02_6_17.csv',\n",
       " '02_6_18.csv',\n",
       " '02_6_19.csv',\n",
       " '02_6_20.csv',\n",
       " '02_6_21.csv',\n",
       " '02_6_24.csv',\n",
       " '02_6_25.csv',\n",
       " '02_6_26.csv',\n",
       " '02_6_27.csv',\n",
       " '02_6_28.csv',\n",
       " '02_6_3.csv',\n",
       " '02_6_4.csv',\n",
       " '02_6_5.csv',\n",
       " '02_6_6.csv',\n",
       " '02_6_7.csv',\n",
       " '02_7_1.csv',\n",
       " '02_7_10.csv',\n",
       " '02_7_11.csv',\n",
       " '02_7_12.csv',\n",
       " '02_7_15.csv',\n",
       " '02_7_16.csv',\n",
       " '02_7_17.csv',\n",
       " '02_7_18.csv',\n",
       " '02_7_19.csv',\n",
       " '02_7_2.csv',\n",
       " '02_7_22.csv',\n",
       " '02_7_23.csv',\n",
       " '02_7_24.csv',\n",
       " '02_7_25.csv',\n",
       " '02_7_26.csv',\n",
       " '02_7_29.csv',\n",
       " '02_7_3.csv',\n",
       " '02_7_30.csv',\n",
       " '02_7_31.csv',\n",
       " '02_7_4.csv',\n",
       " '02_7_5.csv',\n",
       " '02_7_8.csv',\n",
       " '02_7_9.csv',\n",
       " '02_8_1.csv',\n",
       " '02_8_12.csv',\n",
       " '02_8_13.csv',\n",
       " '02_8_14.csv',\n",
       " '02_8_15.csv',\n",
       " '02_8_16.csv',\n",
       " '02_8_19.csv',\n",
       " '02_8_2.csv',\n",
       " '02_8_20.csv',\n",
       " '02_8_21.csv',\n",
       " '02_8_22.csv',\n",
       " '02_8_23.csv',\n",
       " '02_8_26.csv',\n",
       " '02_8_27.csv',\n",
       " '02_8_28.csv',\n",
       " '02_8_29.csv',\n",
       " '02_8_30.csv',\n",
       " '02_8_5.csv',\n",
       " '02_8_6.csv',\n",
       " '02_8_7.csv',\n",
       " '02_8_8.csv',\n",
       " '02_8_9.csv',\n",
       " '02_9_10.csv',\n",
       " '02_9_11.csv',\n",
       " '02_9_12.csv',\n",
       " '02_9_13.csv',\n",
       " '02_9_16.csv',\n",
       " '02_9_17.csv',\n",
       " '02_9_18.csv',\n",
       " '02_9_19.csv',\n",
       " '02_9_2.csv',\n",
       " '02_9_20.csv',\n",
       " '02_9_23.csv',\n",
       " '02_9_24.csv',\n",
       " '02_9_25.csv',\n",
       " '02_9_26.csv',\n",
       " '02_9_27.csv',\n",
       " '02_9_3.csv',\n",
       " '02_9_30.csv',\n",
       " '02_9_4.csv',\n",
       " '02_9_5.csv',\n",
       " '02_9_6.csv',\n",
       " '02_9_9.csv',\n",
       " '03_10_1.csv',\n",
       " '03_10_10.csv',\n",
       " '03_10_13.csv',\n",
       " '03_10_14.csv',\n",
       " '03_10_15.csv',\n",
       " '03_10_16.csv',\n",
       " '03_10_17.csv',\n",
       " '03_10_2.csv',\n",
       " '03_10_20.csv',\n",
       " '03_10_21.csv',\n",
       " '03_10_22.csv',\n",
       " '03_10_23.csv',\n",
       " '03_10_24.csv',\n",
       " '03_10_27.csv',\n",
       " '03_10_28.csv',\n",
       " '03_10_29.csv',\n",
       " '03_10_3.csv',\n",
       " '03_10_30.csv',\n",
       " '03_10_31.csv',\n",
       " '03_10_6.csv',\n",
       " '03_10_7.csv',\n",
       " '03_10_8.csv',\n",
       " '03_10_9.csv',\n",
       " '03_11_10.csv',\n",
       " '03_11_11.csv',\n",
       " '03_11_12.csv',\n",
       " '03_11_13.csv',\n",
       " '03_11_14.csv',\n",
       " '03_11_17.csv',\n",
       " '03_11_18.csv',\n",
       " '03_11_19.csv',\n",
       " '03_11_20.csv',\n",
       " '03_11_21.csv',\n",
       " '03_11_24.csv',\n",
       " '03_11_25.csv',\n",
       " '03_11_26.csv',\n",
       " '03_11_27.csv',\n",
       " '03_11_28.csv',\n",
       " '03_11_3.csv',\n",
       " '03_11_4.csv',\n",
       " '03_11_5.csv',\n",
       " '03_11_6.csv',\n",
       " '03_11_7.csv',\n",
       " '03_12_1.csv',\n",
       " '03_12_10.csv',\n",
       " '03_12_11.csv',\n",
       " '03_12_12.csv',\n",
       " '03_12_15.csv',\n",
       " '03_12_16.csv',\n",
       " '03_12_17.csv',\n",
       " '03_12_18.csv',\n",
       " '03_12_19.csv',\n",
       " '03_12_2.csv',\n",
       " '03_12_22.csv',\n",
       " '03_12_23.csv',\n",
       " '03_12_24.csv',\n",
       " '03_12_25.csv',\n",
       " '03_12_26.csv',\n",
       " '03_12_29.csv',\n",
       " '03_12_3.csv',\n",
       " '03_12_30.csv',\n",
       " '03_12_4.csv',\n",
       " '03_12_5.csv',\n",
       " '03_12_8.csv',\n",
       " '03_12_9.csv',\n",
       " '03_1_10.csv',\n",
       " '03_1_13.csv',\n",
       " '03_1_14.csv',\n",
       " '03_1_15.csv',\n",
       " '03_1_16.csv',\n",
       " '03_1_17.csv',\n",
       " '03_1_2.csv',\n",
       " '03_1_20.csv',\n",
       " '03_1_21.csv',\n",
       " '03_1_22.csv',\n",
       " '03_1_23.csv',\n",
       " '03_1_24.csv',\n",
       " '03_1_27.csv',\n",
       " '03_1_28.csv',\n",
       " '03_1_29.csv',\n",
       " '03_1_3.csv',\n",
       " '03_1_30.csv',\n",
       " '03_1_31.csv',\n",
       " '03_1_6.csv',\n",
       " '03_1_7.csv',\n",
       " '03_1_8.csv',\n",
       " '03_1_9.csv',\n",
       " '03_2_10.csv',\n",
       " '03_2_11.csv',\n",
       " '03_2_12.csv',\n",
       " '03_2_13.csv',\n",
       " '03_2_14.csv',\n",
       " '03_2_17.csv',\n",
       " '03_2_18.csv',\n",
       " '03_2_19.csv',\n",
       " '03_2_20.csv',\n",
       " '03_2_21.csv',\n",
       " '03_2_24.csv',\n",
       " '03_2_25.csv',\n",
       " '03_2_26.csv',\n",
       " '03_2_27.csv',\n",
       " '03_2_28.csv',\n",
       " '03_2_3.csv',\n",
       " '03_2_4.csv',\n",
       " '03_2_5.csv',\n",
       " '03_2_6.csv',\n",
       " '03_2_7.csv',\n",
       " '03_3_10.csv',\n",
       " '03_3_11.csv',\n",
       " '03_3_12.csv',\n",
       " '03_3_13.csv',\n",
       " '03_3_14.csv',\n",
       " '03_3_17.csv',\n",
       " '03_3_18.csv',\n",
       " '03_3_19.csv',\n",
       " '03_3_20.csv',\n",
       " '03_3_21.csv',\n",
       " '03_3_24.csv',\n",
       " '03_3_25.csv',\n",
       " '03_3_26.csv',\n",
       " '03_3_27.csv',\n",
       " '03_3_28.csv',\n",
       " '03_3_3.csv',\n",
       " '03_3_31.csv',\n",
       " '03_3_4.csv',\n",
       " '03_3_5.csv',\n",
       " '03_3_6.csv',\n",
       " '03_3_7.csv',\n",
       " '03_4_1.csv',\n",
       " '03_4_10.csv',\n",
       " '03_4_11.csv',\n",
       " '03_4_14.csv',\n",
       " '03_4_15.csv',\n",
       " '03_4_16.csv',\n",
       " '03_4_17.csv',\n",
       " '03_4_18.csv',\n",
       " '03_4_2.csv',\n",
       " '03_4_21.csv',\n",
       " '03_4_22.csv',\n",
       " '03_4_23.csv',\n",
       " '03_4_24.csv',\n",
       " '03_4_25.csv',\n",
       " '03_4_28.csv',\n",
       " '03_4_29.csv',\n",
       " '03_4_3.csv',\n",
       " '03_4_30.csv',\n",
       " '03_4_4.csv',\n",
       " '03_4_7.csv',\n",
       " '03_4_8.csv',\n",
       " '03_4_9.csv',\n",
       " '03_5_1.csv',\n",
       " '03_5_12.csv',\n",
       " '03_5_13.csv',\n",
       " '03_5_14.csv',\n",
       " '03_5_15.csv',\n",
       " '03_5_16.csv',\n",
       " '03_5_19.csv',\n",
       " '03_5_2.csv',\n",
       " '03_5_20.csv',\n",
       " '03_5_21.csv',\n",
       " '03_5_22.csv',\n",
       " '03_5_23.csv',\n",
       " '03_5_26.csv',\n",
       " '03_5_27.csv',\n",
       " '03_5_28.csv',\n",
       " '03_5_29.csv',\n",
       " '03_5_30.csv',\n",
       " '03_5_5.csv',\n",
       " '03_5_6.csv',\n",
       " '03_5_7.csv',\n",
       " '03_5_8.csv',\n",
       " '03_5_9.csv',\n",
       " '03_6_10.csv',\n",
       " '03_6_11.csv',\n",
       " '03_6_12.csv',\n",
       " '03_6_13.csv',\n",
       " '03_6_16.csv',\n",
       " '03_6_17.csv',\n",
       " '03_6_18.csv',\n",
       " '03_6_19.csv',\n",
       " '03_6_2.csv',\n",
       " '03_6_20.csv',\n",
       " '03_6_23.csv',\n",
       " '03_6_24.csv',\n",
       " '03_6_25.csv',\n",
       " '03_6_26.csv',\n",
       " '03_6_27.csv',\n",
       " '03_6_3.csv',\n",
       " '03_6_30.csv',\n",
       " '03_6_4.csv',\n",
       " '03_6_5.csv',\n",
       " '03_6_6.csv',\n",
       " '03_6_9.csv',\n",
       " '03_7_1.csv',\n",
       " '03_7_10.csv',\n",
       " '03_7_11.csv',\n",
       " '03_7_14.csv',\n",
       " '03_7_15.csv',\n",
       " '03_7_16.csv',\n",
       " '03_7_17.csv',\n",
       " '03_7_18.csv',\n",
       " '03_7_2.csv',\n",
       " '03_7_21.csv',\n",
       " '03_7_22.csv',\n",
       " '03_7_23.csv',\n",
       " '03_7_24.csv',\n",
       " '03_7_25.csv',\n",
       " '03_7_28.csv',\n",
       " '03_7_29.csv',\n",
       " '03_7_3.csv',\n",
       " '03_7_30.csv',\n",
       " '03_7_31.csv',\n",
       " '03_7_4.csv',\n",
       " '03_7_7.csv',\n",
       " '03_7_8.csv',\n",
       " '03_7_9.csv',\n",
       " '03_8_1.csv',\n",
       " '03_8_11.csv',\n",
       " '03_8_12.csv',\n",
       " '03_8_13.csv',\n",
       " '03_8_14.csv',\n",
       " '03_8_15.csv',\n",
       " '03_8_18.csv',\n",
       " '03_8_19.csv',\n",
       " '03_8_20.csv',\n",
       " '03_8_21.csv',\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('C:\\\\Users\\\\jky93\\\\KYdrive\\\\바탕 화면\\\\창업\\\\팩터프로젝트\\\\KRX_price')\n",
    "for filename in os.listdir():\n",
    "    data = pd.read_csv(filename, encoding='cp949')\n",
    "    date = filename[0:2]+filename[3:5]+filename[6:8]\n",
    "    data['종목코드'] = data['종목코드'].map(lambda x:str(x).zfill(6) if x != None else None)\n",
    "    temp = data[['시장구분', '등락률', '거래량', '거래대금', '시가총액', '종가']].set_index('종목코드')\n",
    "    temp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19188e37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
